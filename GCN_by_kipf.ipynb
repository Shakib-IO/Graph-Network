{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Follow This: https://blog.devgenius.io/how-to-train-a-graph-convolutional-network-on-the-cora-dataset-with-pytorch-geometric-847ed5fab9cb"
      ],
      "metadata": {
        "id": "SojRvqQeEXK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github: https://github.com/mnslarcher/cs224w-slides-to-code/blob/main/notebooks/06-graph-neural-networks-1-gnn-model.ipynb"
      ],
      "metadata": {
        "id": "bmhm0yjNEepc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYB72__hy1P3",
        "outputId": "34c3c628-78df-4122-8b7f-9568e489d89e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 3.7 MB/s \n",
            "\u001b[?25h  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 467 kB 4.1 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    # Check if PyTorch Geometric is installed:\n",
        "    import torch_geometric\n",
        "except ImportError:\n",
        "    # If PyTorch Geometric is not installed, install it.\n",
        "    %pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "    %pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "    %pip install -q torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries \n",
        "from typing import Callable, List, Optional, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "from torch import Tensor\n",
        "from torch.optim import Optimizer\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import GCNConv\n",
        "from typing_extensions import Literal, TypedDict"
      ],
      "metadata": {
        "id": "LYs4LoLEzW01"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "dataset = Planetoid(\"/temp/Cora\", name = \"Cora\")\n",
        "num_nodes =  dataset.data.num_nodes\n",
        "num_edges = dataset.data.num_edges // 2\n",
        "train_len = dataset[0].train_mask.sum()\n",
        "val_len = dataset[0].val_mask.sum()\n",
        "test_len = dataset[0].test_mask.sum()\n",
        "other_len = num_nodes - train_len - val_len - test_len\n",
        "print(f\"Dataset: {dataset.name}\")\n",
        "print(f\"Num. nodes: {num_nodes} (train={train_len}, val={val_len}, test={test_len}, other={other_len})\")\n",
        "print(f\"Num. edges: {num_edges}\")\n",
        "print(f\"Num. node features: {dataset.num_node_features}\")\n",
        "print(f\"Num. classes: {dataset.num_classes}\")\n",
        "print(f\"Dataset len.: {dataset.len()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUhB4_Vpz7NZ",
        "outputId": "a0867e5d-965a-4c6c-de5b-ffd72e0403d1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: Cora\n",
            "Num. nodes: 2708 (train=140, val=500, test=1000, other=1068)\n",
            "Num. edges: 5278\n",
            "Num. node features: 1433\n",
            "Num. classes: 7\n",
            "Dataset len.: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"We initialize weights using the initialization described in \n",
        "# Glorot & Bengio (2010) and accordingly (row-)normalize input feature vectors.\"\n",
        "\n",
        "dataset = Planetoid(\"/tmp/Cora\", name=\"Cora\")\n",
        "print(f\"Sum of the row values without normalization: {dataset[0].x.sum(dim = 1)}\")\n",
        "\n",
        "dataset = Planetoid(\"/tmp/Cora\", name=\"Cora\", transform=T.NormalizeFeatures())\n",
        "print(f\"Sum of the row value with normalization: {dataset[0].x.sum(dim=0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1A6b2d37mhF",
        "outputId": "247af395-7af6-4c73-bee9-7f179b36d701"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of the row values without normalization: tensor([ 9., 23., 19.,  ..., 18., 14., 13.])\n",
            "Sum of the row value with normalization: tensor([0.7399, 1.6961, 3.4994,  ..., 0.3656, 3.2537, 0.6146])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GCN\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_node_features: int,\n",
        "        num_classes: int,\n",
        "        hidden_dim: int = 16,\n",
        "        dropout_rate: float = 0.5,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout1 = torch.nn.Dropout(dropout_rate)\n",
        "        self.conv1 = GCNConv(num_node_features, hidden_dim)\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "        self.dropout2 = torch.nn.Dropout(dropout_rate)\n",
        "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Tensor) -> torch.Tensor:\n",
        "        x = self.dropout1(x)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Bi8LsuByAD5v"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GCN:\")\n",
        "GCN(dataset.num_node_features, dataset.num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqkdEo9aD7vX",
        "outputId": "e88dd6f6-28df-4e21-ca34-a68eae42682e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GCN(\n",
              "  (dropout1): Dropout(p=0.5, inplace=False)\n",
              "  (conv1): GCNConv(1433, 16)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (dropout2): Dropout(p=0.5, inplace=False)\n",
              "  (conv2): GCNConv(16, 7)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torchmetrics\n",
        "from torchmetrics import Accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCFcoLbtHFx0",
        "outputId": "c8c2fdf8-649f-4b81-ed34-9b39ba76fa49"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.10.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training and Evaluation Step\n",
        "LossFn = Callable[[Tensor, Tensor], Tensor]\n",
        "Stage = Literal[\"train\", \"val\", \"test\"]\n",
        "\n",
        "\n",
        "def train_step(\n",
        "    model: torch.nn.Module, data: Data, optimizer: torch.optim.Optimizer, loss_fn: LossFn\n",
        ") -> Tuple[float, float]:\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    mask = data.train_mask\n",
        "    logits = model(data.x, data.edge_index)[mask]\n",
        "    preds = logits.argmax(dim=1)\n",
        "    y = data.y[mask]\n",
        "    loss = loss_fn(logits, y)\n",
        "    acc = Accuracy(preds, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item(), acc\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_step(model: torch.nn.Module, data: Data, loss_fn: LossFn, stage: Stage) -> Tuple[float, float]:\n",
        "    model.eval()\n",
        "    mask = getattr(data, f\"{stage}_mask\")\n",
        "    logits = model(data.x, data.edge_index)[mask]\n",
        "    preds = logits.argmax(dim=1)\n",
        "    y = data.y[mask]\n",
        "    loss = loss_fn(logits, y)\n",
        "    acc = Accuracy(preds, y)\n",
        "    return loss.item(), acc"
      ],
      "metadata": {
        "id": "NCsQ41f7EPqm"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HistoryDict(TypedDict):\n",
        "    loss: List[float]\n",
        "    acc: List[float]\n",
        "    val_loss: List[float]\n",
        "    val_acc: List[float]\n",
        "\n",
        "def train(\n",
        "    model: torch.nn.Module,\n",
        "    data: Data,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    loss_fn: LossFn = torch.nn.CrossEntropyLoss(),\n",
        "    max_epochs: int = 100,\n",
        "    early_stopping: int = 10,\n",
        "    print_interval: int = 20,\n",
        "    verbose: bool = True,\n",
        ") -> HistoryDict:\n",
        "    history = {\"loss\": [], \"val_loss\": [], \"acc\": [], \"val_acc\": []}\n",
        "    for epoch in range(max_epochs):\n",
        "        loss, acc = train_step(model, data, optimizer, loss_fn)\n",
        "        val_loss, val_acc = eval_step(model, data, loss_fn, \"val\")\n",
        "        history[\"loss\"].append(loss)\n",
        "        history[\"acc\"].append(acc)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "        # The official implementation in TensorFlow is a little different from what is described in the paper...\n",
        "        if epoch > early_stopping and val_loss > np.mean(history[\"val_loss\"][-(early_stopping + 1) : -1]):\n",
        "            if verbose:\n",
        "                print(\"\\nEarly stopping...\")\n",
        "\n",
        "            break\n",
        "\n",
        "        if verbose and epoch % print_interval == 0:\n",
        "            print(f\"\\nEpoch: {epoch}\\n----------\")\n",
        "            print(f\"Train loss: {loss:.4f} | Train acc: {acc:.4f}\")\n",
        "            print(f\"  Val loss: {val_loss:.4f} |   Val acc: {val_acc:.4f}\")\n",
        "\n",
        "    test_loss, test_acc = eval_step(model, data, loss_fn, \"test\")\n",
        "    if verbose:\n",
        "        print(f\"\\nEpoch: {epoch}\\n----------\")\n",
        "        print(f\"Train loss: {loss:.4f} | Train acc: {acc:.4f}\")\n",
        "        print(f\"  Val loss: {val_loss:.4f} |   Val acc: {val_acc:.4f}\")\n",
        "        print(f\" Test loss: {test_loss:.4f} |  Test acc: {test_acc:.4f}\")\n",
        "\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "UFlePzTlKfKG"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history: HistoryDict, title: str, font_size: Optional[int] = 14) -> None:\n",
        "    plt.suptitle(title, fontsize=font_size)\n",
        "    ax1 = plt.subplot(121)\n",
        "    ax1.set_title(\"Loss\")\n",
        "    ax1.plot(history[\"loss\"], label=\"train\")\n",
        "    ax1.plot(history[\"val_loss\"], label=\"val\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2 = plt.subplot(122)\n",
        "    ax2.set_title(\"Accuracy\")\n",
        "    ax2.plot(history[\"acc\"], label=\"train\")\n",
        "    ax2.plot(history[\"val_acc\"], label=\"val\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    ax2.legend()"
      ],
      "metadata": {
        "id": "SvF8Wv85LkzB"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "MAX_EPOCHS = 100\n",
        "LEARNING_RATE = 0.01\n",
        "WEIGHT_DECAY = 5e-4\n",
        "EARLY_STOPPING = 10\n",
        "\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = GCN(dataset.num_node_features, dataset.num_classes).to(device)\n",
        "data = dataset[0].to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "history = train(model, data, optimizer, max_epochs=MAX_EPOCHS, early_stopping=EARLY_STOPPING)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plot_history(history, \"GCN\")"
      ],
      "metadata": {
        "id": "PSgsXTRoLsPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6inD_T03Lwu0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}